{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":269740,"sourceType":"datasetVersion","datasetId":113003},{"sourceId":4243451,"sourceType":"datasetVersion","datasetId":32526},{"sourceId":9601762,"sourceType":"datasetVersion","datasetId":5668978},{"sourceId":10280684,"sourceType":"datasetVersion","datasetId":4892370}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-02-21T10:24:11.885025Z","iopub.execute_input":"2025-02-21T10:24:11.885303Z","iopub.status.idle":"2025-02-21T10:24:12.217947Z","shell.execute_reply.started":"2025-02-21T10:24:11.885280Z","shell.execute_reply":"2025-02-21T10:24:12.217076Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/news-category-dataset/News_Category_Dataset_v3.json\n/kaggle/input/personal-data/high_contrast.jpeg\n/kaggle/input/personal-data/1694354392633.jpg\n/kaggle/input/personal-data/Translation of banglaNewsData.xlsx\n/kaggle/input/personal-data/hindu.csv\n/kaggle/input/personal-data/political.txt\n/kaggle/input/personal-data/BanglaBlendCleanedData.xlsx\n/kaggle/input/personal-data/result31.csv\n/kaggle/input/personal-data/low_contrast_cat.jpeg\n/kaggle/input/personal-data/IMG_3661.jpg\n/kaggle/input/personal-data/Translator Dataset/Word.xlsx\n/kaggle/input/personal-data/Translator Dataset/Clause.xlsx\n/kaggle/input/personal-data/Translator Dataset/Sentence.xlsx\n/kaggle/input/sarcastic-comments-on-reddit/train-balanced-sarcasm.csv\n/kaggle/input/sarcasm/reddit_test.jsonl\n/kaggle/input/sarcasm/sarcasm_detection_shared_task_twitter_training.jsonl\n/kaggle/input/sarcasm/twitter_test.jsonl\n/kaggle/input/sarcasm/sarcasm_detection_shared_task_reddit_training.jsonl\n/kaggle/input/sarcasm/Ben-Sarc_ Bengali Sarcasm Detection Corpus.xlsx\n/kaggle/input/sarcasm/sarcasm_v2/RQ-sarc-notsarc.csv\n/kaggle/input/sarcasm/sarcasm_v2/GEN-sarc-notsarc.csv\n/kaggle/input/sarcasm/sarcasm_v2/HYP-sarc-notsarc.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd.read_excel('/kaggle/input/sarcasm/Ben-Sarc_ Bengali Sarcasm Detection Corpus.xlsx')\n\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:24:14.961191Z","iopub.execute_input":"2025-02-21T10:24:14.961614Z","iopub.status.idle":"2025-02-21T10:24:17.252134Z","shell.execute_reply.started":"2025-02-21T10:24:14.961589Z","shell.execute_reply":"2025-02-21T10:24:17.251349Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   id                                               Text  Polarity\n0   0  শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...         1\n1   2                             সাথে আছে বুক ভরা চুল ।         1\n2   4  ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...         1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>Text</th>\n      <th>Polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>সাথে আছে বুক ভরা চুল ।</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"df.rename(columns={\"Text\": \"text\"}, inplace=True)\ndf.rename(columns={\"Polarity\": \"label\"}, inplace=True)\n\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:24:17.253114Z","iopub.execute_input":"2025-02-21T10:24:17.253454Z","iopub.status.idle":"2025-02-21T10:24:17.263625Z","shell.execute_reply.started":"2025-02-21T10:24:17.253432Z","shell.execute_reply":"2025-02-21T10:24:17.262781Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   id                                               text  label\n0   0  শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...      1\n1   2                             সাথে আছে বুক ভরা চুল ।      1\n2   4  ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>সাথে আছে বুক ভরা চুল ।</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Apply Normalizer","metadata":{}},{"cell_type":"code","source":"pip install git+https://github.com/csebuetnlp/normalizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:33.408627Z","iopub.execute_input":"2025-02-21T10:25:33.409003Z","iopub.status.idle":"2025-02-21T10:25:44.084940Z","shell.execute_reply.started":"2025-02-21T10:25:33.408973Z","shell.execute_reply":"2025-02-21T10:25:44.084029Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/csebuetnlp/normalizer\n  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-fcb7pqeh\n  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-fcb7pqeh\n  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from normalizer==0.0.1) (2024.9.11)\nCollecting emoji==1.4.2 (from normalizer==0.0.1)\n  Downloading emoji-1.4.2.tar.gz (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting ftfy==6.0.3 (from normalizer==0.0.1)\n  Downloading ftfy-6.0.3.tar.gz (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.13)\nBuilding wheels for collected packages: normalizer, emoji, ftfy\n  Building wheel for normalizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6860 sha256=67004586914494c3b031ff1ee8c278ced135375f8c914c2d20d6c3cecaac313a\n  Stored in directory: /tmp/pip-ephem-wheel-cache-49p9x4u2/wheels/2e/79/9c/cd96d490298305d51d2da11484bb2c25fd1f759a6906708282\n  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186456 sha256=063d4df6d618b5d32e0d73b37523b3a1db04bd3551acedfdd7d3e0f024758ebf\n  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41930 sha256=479e16bede498cae6455f7636daf56d764f3c1ba9a62a16aec0ebd6d7cfec08b\n  Stored in directory: /root/.cache/pip/wheels/92/8e/16/c1e4d4d65685d71085e4e27b44d6ed880b0559474c9ee4ff66\nSuccessfully built normalizer emoji ftfy\nInstalling collected packages: emoji, ftfy, normalizer\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.14.0\n    Uninstalling emoji-2.14.0:\n      Successfully uninstalled emoji-2.14.0\nSuccessfully installed emoji-1.4.2 ftfy-6.0.3 normalizer-0.0.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from normalizer import normalize \nfrom tqdm import tqdm\n\ntqdm.pandas()\ndf['text'] = df['text'].progress_apply(normalize)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:44.086039Z","iopub.execute_input":"2025-02-21T10:25:44.086336Z","iopub.status.idle":"2025-02-21T10:25:48.414318Z","shell.execute_reply.started":"2025-02-21T10:25:44.086313Z","shell.execute_reply":"2025-02-21T10:25:48.413526Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 25636/25636 [00:04<00:00, 6119.98it/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df['rtext'] = df['text'].progress_apply(lambda x: ' '.join(x.split()[::-1]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:48.415849Z","iopub.execute_input":"2025-02-21T10:25:48.416062Z","iopub.status.idle":"2025-02-21T10:25:48.495030Z","shell.execute_reply.started":"2025-02-21T10:25:48.416042Z","shell.execute_reply":"2025-02-21T10:25:48.493992Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 25636/25636 [00:00<00:00, 360821.53it/s]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"ri = 2\ndf['text'][ri], df['label'][ri]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:48.495976Z","iopub.execute_input":"2025-02-21T10:25:48.496264Z","iopub.status.idle":"2025-02-21T10:25:48.501629Z","shell.execute_reply.started":"2025-02-21T10:25:48.496242Z","shell.execute_reply":"2025-02-21T10:25:48.500767Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"('ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পোস্ট দিয়েছেন এবং আপনি রঙিন দুনিয়া দেখছেন না । কারণ আপনার ফ্রেম রঙিন হলেও গ্লাসটা কিন্তু কালো । আমি কিন্তু রঙিন দুনিয়ার একজন রঙমিস্ত্রি তাই বিষয়টি এড়িয়ে যেতে পারলাম না ।',\n 1)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:48.893521Z","iopub.execute_input":"2025-02-21T10:25:48.893833Z","iopub.status.idle":"2025-02-21T10:25:48.903957Z","shell.execute_reply.started":"2025-02-21T10:25:48.893808Z","shell.execute_reply":"2025-02-21T10:25:48.903222Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n1    12818\n0    12818\nName: count, dtype: int64"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:51.251863Z","iopub.execute_input":"2025-02-21T10:25:51.252196Z","iopub.status.idle":"2025-02-21T10:25:51.256975Z","shell.execute_reply.started":"2025-02-21T10:25:51.252151Z","shell.execute_reply":"2025-02-21T10:25:51.256136Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(25636, 4)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Assume 'df' is your DataFrame and 'label_column' is the column you want to stratify on\n# df, _ = train_test_split(df, train_size=1000, stratify=df['label'], random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:51.595721Z","iopub.execute_input":"2025-02-21T10:25:51.595986Z","iopub.status.idle":"2025-02-21T10:25:52.747908Z","shell.execute_reply.started":"2025-02-21T10:25:51.595964Z","shell.execute_reply":"2025-02-21T10:25:52.746998Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(df.shape)\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:55.589092Z","iopub.execute_input":"2025-02-21T10:25:55.589610Z","iopub.status.idle":"2025-02-21T10:25:55.599739Z","shell.execute_reply.started":"2025-02-21T10:25:55.589580Z","shell.execute_reply":"2025-02-21T10:25:55.598747Z"}},"outputs":[{"name":"stdout","text":"(25636, 4)\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   id                                               text  label  \\\n0   0  শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...      1   \n1   2                             সাথে আছে বুক ভরা চুল ।      1   \n2   4  ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...      1   \n\n                                               rtext  \n0  ছিলাম বসে কিনে বি এম করে বিক্রি ঘোড়া পাড়া ডি...  \n1                             । চুল ভরা বুক আছে সাথে  \n2  । না পারলাম যেতে এড়িয়ে বিষয়টি তাই রঙমিস্ত্র...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>label</th>\n      <th>rtext</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...</td>\n      <td>1</td>\n      <td>ছিলাম বসে কিনে বি এম করে বিক্রি ঘোড়া পাড়া ডি...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>সাথে আছে বুক ভরা চুল ।</td>\n      <td>1</td>\n      <td>। চুল ভরা বুক আছে সাথে</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...</td>\n      <td>1</td>\n      <td>। না পারলাম যেতে এড়িয়ে বিষয়টি তাই রঙমিস্ত্র...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Training On MLM","metadata":{}},{"cell_type":"code","source":"sentences = df.text.values.tolist()\n\nlabels = df.label.values.tolist()\n\nprint(len(sentences))\n\nprint(len(labels))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:25:58.198494Z","iopub.execute_input":"2025-02-21T10:25:58.198829Z","iopub.status.idle":"2025-02-21T10:25:58.205327Z","shell.execute_reply.started":"2025-02-21T10:25:58.198802Z","shell.execute_reply":"2025-02-21T10:25:58.204406Z"}},"outputs":[{"name":"stdout","text":"25636\n25636\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"sentences[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:00.326671Z","iopub.execute_input":"2025-02-21T10:26:00.326974Z","iopub.status.idle":"2025-02-21T10:26:00.331962Z","shell.execute_reply.started":"2025-02-21T10:26:00.326951Z","shell.execute_reply":"2025-02-21T10:26:00.331262Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এক মাত্র ডিম পাড়া ঘোড়া বিক্রি করে এম বি কিনে বসে ছিলাম'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Specify the model path\n\n# model_path = \"csebuetnlp/banglabert_large\"\nmodel_path = \"csebuetnlp/banglabert\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:00.478830Z","iopub.execute_input":"2025-02-21T10:26:00.479117Z","iopub.status.idle":"2025-02-21T10:26:00.482441Z","shell.execute_reply.started":"2025-02-21T10:26:00.479093Z","shell.execute_reply":"2025-02-21T10:26:00.481598Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# First, disable wandb\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_SILENT\"] = \"true\"\n\n# Import required libraries\nfrom transformers import (\n    AutoModelForMaskedLM,\n    AutoTokenizer,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:02.339831Z","iopub.execute_input":"2025-02-21T10:26:02.340146Z","iopub.status.idle":"2025-02-21T10:26:16.268248Z","shell.execute_reply.started":"2025-02-21T10:26:02.340121Z","shell.execute_reply":"2025-02-21T10:26:16.267568Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nfrom datasets import Dataset\n\n# Prepare your text data\n# your_unlabeled_texts = df.parent_comm.values.tolist()\n\n# Create dataset\ndataset = Dataset.from_dict({\"text\": sentences[:500]})\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n\n# Tokenize function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=64,\n        return_special_tokens_mask=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:16.269298Z","iopub.execute_input":"2025-02-21T10:26:16.269866Z","iopub.status.idle":"2025-02-21T10:26:17.068062Z","shell.execute_reply.started":"2025-02-21T10:26:16.269829Z","shell.execute_reply":"2025-02-21T10:26:17.067227Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4506a53e4f624929949cf7c6ea5a2be9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17cdd092e7eb4cc99d2f5d736fed302f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a676100a474298bd209ea81eff73c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e188b177a46e4bee94949145cafcf0c3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Print the original sentence.\nprint(' Original: ', sentences[0])\n\n# Print the sentence split into tokens.\nprint('Tokenized: ', tokenizer.tokenize(sentences[0]))\n\n# Print the sentence mapped to token ids.\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:17.069522Z","iopub.execute_input":"2025-02-21T10:26:17.069813Z","iopub.status.idle":"2025-02-21T10:26:17.078496Z","shell.execute_reply.started":"2025-02-21T10:26:17.069790Z","shell.execute_reply":"2025-02-21T10:26:17.077728Z"}},"outputs":[{"name":"stdout","text":" Original:  শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এক মাত্র ডিম পাড়া ঘোড়া বিক্রি করে এম বি কিনে বসে ছিলাম\nTokenized:  ['শুধু', 'মাত্র', 'এই', 'পোস্টে', 'কমেন্ট', 'করার', 'জন্য', 'বাড়ির', 'এক', 'মাত্র', 'ডিম', 'পাড়া', 'ঘোড়া', 'বিক্রি', 'করে', 'এম', 'বি', 'কিনে', 'বসে', 'ছিলাম']\nToken IDs:  [1289, 1768, 830, 12098, 8764, 1121, 900, 2095, 788, 1768, 5270, 9224, 8046, 3149, 792, 1611, 797, 3597, 1346, 3285]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Tokenize dataset\ntokenized_dataset = dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=dataset.column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:17.079154Z","iopub.execute_input":"2025-02-21T10:26:17.079380Z","iopub.status.idle":"2025-02-21T10:26:17.178759Z","shell.execute_reply.started":"2025-02-21T10:26:17.079361Z","shell.execute_reply":"2025-02-21T10:26:17.177761Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0151b34f4ede478e95abaa3ea0785b20"}},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## Load MLM Model","metadata":{}},{"cell_type":"code","source":"print(model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:17.179774Z","iopub.execute_input":"2025-02-21T10:26:17.180105Z","iopub.status.idle":"2025-02-21T10:26:17.184497Z","shell.execute_reply.started":"2025-02-21T10:26:17.180070Z","shell.execute_reply":"2025-02-21T10:26:17.183586Z"}},"outputs":[{"name":"stdout","text":"csebuetnlp/banglabert\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Load model for MLM\nmlm_model = AutoModelForMaskedLM.from_pretrained(model_path)\n# Create data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=True,\n    mlm_probability=0.15\n)\n\nmlm_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:17.185445Z","iopub.execute_input":"2025-02-21T10:26:17.185767Z","iopub.status.idle":"2025-02-21T10:26:22.706069Z","shell.execute_reply.started":"2025-02-21T10:26:17.185733Z","shell.execute_reply":"2025-02-21T10:26:22.705418Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4dac6bdb84804aab9a0de37bdc0f0e1a"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForMaskedLM were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['generator_lm_head.bias', 'generator_predictions.LayerNorm.bias', 'generator_predictions.LayerNorm.weight', 'generator_predictions.dense.bias', 'generator_predictions.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"ElectraForMaskedLM(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (generator_predictions): ElectraGeneratorPredictions(\n    (activation): GELUActivation()\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n  )\n  (generator_lm_head): Linear(in_features=768, out_features=32000, bias=True)\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"Epochs = 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:26.470410Z","iopub.execute_input":"2025-02-21T10:26:26.470712Z","iopub.status.idle":"2025-02-21T10:26:26.474256Z","shell.execute_reply.started":"2025-02-21T10:26:26.470688Z","shell.execute_reply":"2025-02-21T10:26:26.473405Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"# Setup training arguments without automatic saving.\ntraining_args = TrainingArguments(\n    output_dir=\"./mlm_pretrained_model\",  # still required for logs etc.\n    report_to=[],                         # Disable all integrations\n    num_train_epochs=Epochs,\n    per_device_train_batch_size=16,\n    save_strategy=\"no\",                   # Disable automatic saving\n    logging_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:26.654028Z","iopub.execute_input":"2025-02-21T10:26:26.654273Z","iopub.status.idle":"2025-02-21T10:26:26.791475Z","shell.execute_reply.started":"2025-02-21T10:26:26.654252Z","shell.execute_reply":"2025-02-21T10:26:26.790788Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# Initialize trainer\ntrainer = Trainer(\n    model=mlm_model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:29.217783Z","iopub.execute_input":"2025-02-21T10:26:29.218102Z","iopub.status.idle":"2025-02-21T10:26:29.565901Z","shell.execute_reply.started":"2025-02-21T10:26:29.218074Z","shell.execute_reply":"2025-02-21T10:26:29.565249Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# Start Training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:30.038608Z","iopub.execute_input":"2025-02-21T10:26:30.038876Z","iopub.status.idle":"2025-02-21T10:26:43.353520Z","shell.execute_reply.started":"2025-02-21T10:26:30.038854Z","shell.execute_reply":"2025-02-21T10:26:43.352811Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 00:10, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>16</td>\n      <td>10.188900</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>8.724800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=32, training_loss=9.45680570602417, metrics={'train_runtime': 12.1741, 'train_samples_per_second': 82.141, 'train_steps_per_second': 2.629, 'total_flos': 32901169152000.0, 'train_loss': 9.45680570602417, 'epoch': 2.0})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Save the model\n\nimport os\nimport torch\n\ndef save_model_contiguous(model, output_dir):\n    os.makedirs(output_dir, exist_ok=True)\n    state_dict = model.state_dict()\n    contiguous_state_dict = {k: v.contiguous() for k, v in state_dict.items()}\n    torch.save(contiguous_state_dict, os.path.join(output_dir, \"pytorch_model.bin\"))\n    # If your model has a configuration saved, also save it:\n    if hasattr(model, \"config\"):\n        model.config.save_pretrained(output_dir)\n    print(f\"Model saved to {output_dir} (all tensors are now contiguous)\")\n\n# Example usage:\noutput_dir = \"./mlm_pretrained_model\"\nsave_model_contiguous(mlm_model, output_dir)\n\n\n# trainer.save_model(\"./mlm_pretrained_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:47.549067Z","iopub.execute_input":"2025-02-21T10:26:47.549384Z","iopub.status.idle":"2025-02-21T10:26:48.206900Z","shell.execute_reply.started":"2025-02-21T10:26:47.549360Z","shell.execute_reply":"2025-02-21T10:26:48.205980Z"}},"outputs":[{"name":"stdout","text":"Model saved to ./mlm_pretrained_model (all tensors are now contiguous)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# # Now you can load this for classification\n# model = AutoModelForSequenceClassification.from_pretrained(\n#     \"./mlm_pretrained_model\",\n#     num_labels=2,\n#     output_attentions=False,\n#     output_hidden_states=False,\n# )\n\nnew_model_path = \"/kaggle/working/mlm_pretrained_model\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:26:56.744697Z","iopub.execute_input":"2025-02-21T10:26:56.745002Z","iopub.status.idle":"2025-02-21T10:26:56.748661Z","shell.execute_reply.started":"2025-02-21T10:26:56.744979Z","shell.execute_reply":"2025-02-21T10:26:56.747873Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# import pandas as pd\n# import json\n\n# # Read the JSONL file\n# data_train = []\n# with open('/kaggle/input/sarcasm/sarcasm_detection_shared_task_reddit_training.jsonl', 'r') as f:\n#     for line in f:\n#         # Parse each line as JSON and append to list\n#         data_train.append(json.loads(line))\n# data_twitter_df = pd.DataFrame(data_train)\n# # Convert list of dictionaries to Data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_twitter_df['label']=data_twitter_df['label'].map({'SARCASM':0,'NOT_SARCASM':1})\n# data_twitter_df=data_twitter_df.rename(columns={'context':'parent_comment','response':'comment'})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_twitter_df.head(3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_twitter_df['comment']=data_twitter_df['comment'].astype(str)\n# data_twitter_df['parent_comment']=data_twitter_df['parent_comment'].astype(str)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\n\nclass MultiFeatureDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.comment = dataframe['comment']\n        self.parent=dataframe['parent_comment']\n        self.labels = dataframe.label\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        if index >= len(self.data):\n            raise IndexError(f\"Index {index} out of range\")\n        comment = str(self.comment.iloc[index])\n        parent = str(self.parent.iloc[index])\n        label = self.labels.iloc[index]\n\n\n \n        comment_encoding = self.tokenizer.encode_plus(\n            comment,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        parent_encoding = self.tokenizer.encode_plus(\n            parent,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n        'input_ids1': parent_encoding['input_ids'].flatten(),\n        'attention_mask1': parent_encoding['attention_mask'].flatten(),\n        'input_ids2': comment_encoding['input_ids'].flatten(),\n        'attention_mask2': comment_encoding['attention_mask'].flatten(),\n        'labels': torch.tensor(label, dtype=torch.long)\n    }","metadata":{"execution":{"iopub.status.busy":"2025-02-21T10:27:00.409907Z","iopub.execute_input":"2025-02-21T10:27:00.410244Z","iopub.status.idle":"2025-02-21T10:27:00.417026Z","shell.execute_reply.started":"2025-02-21T10:27:00.410213Z","shell.execute_reply":"2025-02-21T10:27:00.416272Z"},"trusted":true},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# print(data_twitter_df.shape)\n# data_twitter_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:02.204286Z","iopub.execute_input":"2025-02-21T10:27:02.204602Z","iopub.status.idle":"2025-02-21T10:27:02.208040Z","shell.execute_reply.started":"2025-02-21T10:27:02.204578Z","shell.execute_reply":"2025-02-21T10:27:02.207271Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# df.rename(columns={\"Polarity\": \"label\"}, inplace=True)\ndf.rename(columns={\"text\": \"parent_comment\"}, inplace=True)\ndf.rename(columns={\"rtext\": \"comment\"}, inplace=True)\n\ndf.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:02.485744Z","iopub.execute_input":"2025-02-21T10:27:02.486059Z","iopub.status.idle":"2025-02-21T10:27:02.555850Z","shell.execute_reply.started":"2025-02-21T10:27:02.486034Z","shell.execute_reply":"2025-02-21T10:27:02.555089Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"   id                                     parent_comment  label  \\\n0   0  শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...      1   \n1   2                             সাথে আছে বুক ভরা চুল ।      1   \n2   4  ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...      1   \n\n                                             comment  \n0  ছিলাম বসে কিনে বি এম করে বিক্রি ঘোড়া পাড়া ডি...  \n1                             । চুল ভরা বুক আছে সাথে  \n2  । না পারলাম যেতে এড়িয়ে বিষয়টি তাই রঙমিস্ত্র...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>parent_comment</th>\n      <th>label</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>শুধু মাত্র এই পোস্টে কমেন্ট করার জন্য বাড়ির এ...</td>\n      <td>1</td>\n      <td>ছিলাম বসে কিনে বি এম করে বিক্রি ঘোড়া পাড়া ডি...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>সাথে আছে বুক ভরা চুল ।</td>\n      <td>1</td>\n      <td>। চুল ভরা বুক আছে সাথে</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>ভাই মিথ্যা কথা বইলেন না আপনি ভিপিএন ইউজ করে পো...</td>\n      <td>1</td>\n      <td>। না পারলাম যেতে এড়িয়ে বিষয়টি তাই রঙমিস্ত্র...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"data_twitter_df = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:04.555555Z","iopub.execute_input":"2025-02-21T10:27:04.555842Z","iopub.status.idle":"2025-02-21T10:27:04.563573Z","shell.execute_reply.started":"2025-02-21T10:27:04.555821Z","shell.execute_reply":"2025-02-21T10:27:04.562757Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"n_samples = 1000  # Change this as needed\n\nsampled_df = data_twitter_df.sample(n=n_samples, random_state=42) if len(data_twitter_df) > n_samples else data_twitter_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:04.863120Z","iopub.execute_input":"2025-02-21T10:27:04.863459Z","iopub.status.idle":"2025-02-21T10:27:04.872380Z","shell.execute_reply.started":"2025-02-21T10:27:04.863433Z","shell.execute_reply":"2025-02-21T10:27:04.871571Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, eval_df = train_test_split(sampled_df, test_size=0.2, random_state=42, stratify=sampled_df['label'])\n\nprint(len(train_df), len(eval_df))","metadata":{"execution":{"iopub.status.busy":"2025-02-21T10:27:05.156824Z","iopub.execute_input":"2025-02-21T10:27:05.157079Z","iopub.status.idle":"2025-02-21T10:27:05.165377Z","shell.execute_reply.started":"2025-02-21T10:27:05.157058Z","shell.execute_reply":"2025-02-21T10:27:05.164625Z"},"trusted":true},"outputs":[{"name":"stdout","text":"800 200\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"train_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:11.998981Z","iopub.execute_input":"2025-02-21T10:27:11.999329Z","iopub.status.idle":"2025-02-21T10:27:12.007602Z","shell.execute_reply.started":"2025-02-21T10:27:11.999300Z","shell.execute_reply":"2025-02-21T10:27:12.006727Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"         id                                     parent_comment  label  \\\n3707   8052  মানুষ এর পোস্টে এতো হা হা দেই । তো এর লজ্জা কর...      1   \n738    1603  জি আমরা সবাই বাসায় থাকবো কিন্তু আপনি ক্লাব এ ...      1   \n5068  11156  ইনি মার্কস জুকারবার্গের ছেলে বলে শেষ পর্যন্ত দ...      1   \n\n                                                comment  \n3707  । করতে পোস্ট করেনা লজ্জা এর তো । দেই হা হা এতো...  \n738   । থাইকেন এ ক্লাব আপনি কিন্তু থাকবো বাসায় সবাই...  \n5068  বসে করে দাবি পর্যন্ত শেষ বলে ছেলে জুকারবার্গের...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>parent_comment</th>\n      <th>label</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3707</th>\n      <td>8052</td>\n      <td>মানুষ এর পোস্টে এতো হা হা দেই । তো এর লজ্জা কর...</td>\n      <td>1</td>\n      <td>। করতে পোস্ট করেনা লজ্জা এর তো । দেই হা হা এতো...</td>\n    </tr>\n    <tr>\n      <th>738</th>\n      <td>1603</td>\n      <td>জি আমরা সবাই বাসায় থাকবো কিন্তু আপনি ক্লাব এ ...</td>\n      <td>1</td>\n      <td>। থাইকেন এ ক্লাব আপনি কিন্তু থাকবো বাসায় সবাই...</td>\n    </tr>\n    <tr>\n      <th>5068</th>\n      <td>11156</td>\n      <td>ইনি মার্কস জুকারবার্গের ছেলে বলে শেষ পর্যন্ত দ...</td>\n      <td>1</td>\n      <td>বসে করে দাবি পর্যন্ত শেষ বলে ছেলে জুকারবার্গের...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"train_df.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:13.017339Z","iopub.execute_input":"2025-02-21T10:27:13.017681Z","iopub.status.idle":"2025-02-21T10:27:13.023710Z","shell.execute_reply.started":"2025-02-21T10:27:13.017654Z","shell.execute_reply":"2025-02-21T10:27:13.022944Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"label\n1    418\n0    382\nName: count, dtype: int64"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"eval_df.label.value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:14.339218Z","iopub.execute_input":"2025-02-21T10:27:14.339633Z","iopub.status.idle":"2025-02-21T10:27:14.346950Z","shell.execute_reply.started":"2025-02-21T10:27:14.339595Z","shell.execute_reply":"2025-02-21T10:27:14.345803Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"label\n1    104\n0     96\nName: count, dtype: int64"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"mlm_model.config.hidden_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:14.620706Z","iopub.execute_input":"2025-02-21T10:27:14.620954Z","iopub.status.idle":"2025-02-21T10:27:14.625954Z","shell.execute_reply.started":"2025-02-21T10:27:14.620933Z","shell.execute_reply":"2025-02-21T10:27:14.625237Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"768"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"from transformers import AutoConfig, AutoModel, AutoTokenizer, logging\nimport torch\nimport torch.nn as nn\n\n# Suppress unnecessary warnings\nlogging.set_verbosity_error()\n\nclass CustomEncoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_attention_heads, dropout_rate=0.3):\n        super(CustomEncoderLayer, self).__init__()\n        self.self_attention = nn.MultiheadAttention(hidden_size, num_attention_heads, dropout=dropout_rate)\n        self.cross_attention = nn.MultiheadAttention(hidden_size, num_attention_heads, dropout=dropout_rate)\n        self.feed_forward = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size * 4),\n            nn.ReLU(),\n            nn.Linear(hidden_size * 4, hidden_size)\n        )\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n        self.layer_norm3 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, x, y, x_mask, y_mask):\n        # Self-attention on x\n        temp1 = x\n        self_attn_output, _ = self.self_attention(x, x, x)\n        x = self.layer_norm1(temp1 + self.dropout(self_attn_output))\n        \n        # Cross-attention from x to y\n        cross_attn_output, cross_attn_weights = self.cross_attention(x, y, y)\n        x = self.layer_norm2(temp1 + self.dropout(cross_attn_output))\n        \n        # Feed forward\n        ff_output = self.feed_forward(x)\n        x = self.layer_norm3(temp1 + self.dropout(ff_output))\n        \n        return x, cross_attn_weights\n\nclass CustomDualBanglaBERTModel(nn.Module):\n    def __init__(self, model_checkpoint, num_layers=2, hidden_size=768, num_attention_heads=8, dropout_rate=0.1):\n        super(CustomDualBanglaBERTModel, self).__init__()\n        \n        # Load BanglaBERT Electra-based models\n        self.bert1 = AutoModel.from_pretrained(model_checkpoint)\n        self.bert2 = AutoModel.from_pretrained(model_checkpoint)\n        \n        # Custom encoder layers\n        self.encoder_layers = nn.ModuleList([\n            CustomEncoderLayer(hidden_size, num_attention_heads, dropout_rate)\n            for _ in range(num_layers)\n        ])\n        \n        # Classification head\n        self.fc = nn.Linear(hidden_size * 2, 2)\n\n    def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n        # Get embeddings from two BERT models\n        output1 = self.bert1(input_ids=input_ids1, attention_mask=attention_mask1).last_hidden_state\n        output2 = self.bert2(input_ids=input_ids2, attention_mask=attention_mask2).last_hidden_state\n        \n        # Attention masks for padding tokens\n        x_mask = ~attention_mask1.bool()\n        y_mask = ~attention_mask2.bool()\n        \n        # Apply encoder layers\n        for layer in self.encoder_layers:\n            output1, _ = layer(output1, output2, x_mask, y_mask)\n            output2, cross_attn_weights = layer(output2, output1, y_mask, x_mask)\n        \n        # Pool the outputs from both models\n        pooled_output1 = torch.mean(output1, dim=1)\n        pooled_output2 = torch.mean(output2, dim=1)\n        \n        # Combine and classify\n        combined = torch.cat((pooled_output1, pooled_output2), dim=1)\n        return self.fc(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:27:18.279634Z","iopub.execute_input":"2025-02-21T10:27:18.279931Z","iopub.status.idle":"2025-02-21T10:27:18.291057Z","shell.execute_reply.started":"2025-02-21T10:27:18.279907Z","shell.execute_reply":"2025-02-21T10:27:18.290276Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# Example usage\nmodel_checkpoint = 'csebuetnlp/banglabert'\n# model_checkpoint = \"/kaggle/working/mlm_pretrained_model\"\nmodel = CustomDualBanglaBERTModel(model_checkpoint=model_checkpoint)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model.to(device)\nmodel.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:15.957072Z","iopub.execute_input":"2025-02-21T10:32:15.957417Z","iopub.status.idle":"2025-02-21T10:32:16.788518Z","shell.execute_reply.started":"2025-02-21T10:32:15.957393Z","shell.execute_reply":"2025-02-21T10:32:16.787743Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"CustomDualBanglaBERTModel(\n  (bert1): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (bert2): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (encoder_layers): ModuleList(\n    (0-1): 2 x CustomEncoderLayer(\n      (self_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (cross_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (feed_forward): Sequential(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n      )\n      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (fc): Linear(in_features=1536, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"# Calculate the total number of trainable parameters\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Each parameter takes 4 bytes (float32)\nparam_size_in_bytes = trainable_params * 4\n\n# Convert to GB\nparam_size_in_gb = param_size_in_bytes / (1024**3)\nprint(f\"Approximate Model Size in Memory: {param_size_in_gb:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:21.196622Z","iopub.execute_input":"2025-02-21T10:32:21.197062Z","iopub.status.idle":"2025-02-21T10:32:21.203916Z","shell.execute_reply.started":"2025-02-21T10:32:21.197027Z","shell.execute_reply":"2025-02-21T10:32:21.202888Z"}},"outputs":[{"name":"stdout","text":"Approximate Model Size in Memory: 0.89 GB\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# import torch\n# import torch.nn as nn\n# from transformers import  BertConfig,AutoModel\n\n# class CustomEncoderLayer(nn.Module):\n#     def __init__(self, hidden_size, num_attention_heads, dropout_rate=0.1):\n#         super(CustomEncoderLayer, self).__init__()\n#         self.self_attention = nn.MultiheadAttention(hidden_size, num_attention_heads, dropout=dropout_rate)\n#         self.cross_attention = nn.MultiheadAttention(hidden_size, num_attention_heads, dropout=dropout_rate)\n#         self.feed_forward = nn.Sequential(\n#             nn.Linear(hidden_size, hidden_size * 4),\n#             nn.ReLU(),\n#             nn.Linear(hidden_size * 4, hidden_size)\n#         )\n#         self.layer_norm1 = nn.LayerNorm(hidden_size)\n#         self.layer_norm2 = nn.LayerNorm(hidden_size)\n#         self.layer_norm3 = nn.LayerNorm(hidden_size)\n#         self.dropout = nn.Dropout(dropout_rate)\n\n#     def forward(self, x, y):\n#         # Self-attention on x\n#         e=x\n#         self_attn_output, _ = self.self_attention(x, x, x)\n#         x = self.layer_norm1(e + self.dropout(self_attn_output))\n        \n#         # Cross-attention from x to y\n#         cross_attn_output, cross_attn_weights = self.cross_attention(x, y, y)\n#         x = self.layer_norm2(e + self.dropout(cross_attn_output))\n        \n#         # Feed forward\n#         ff_output = self.feed_forward(x)\n#         x = self.layer_norm3(e + self.dropout(ff_output))\n        \n#         return x, cross_attn_weights\n\n# class CustomDualBERTModel(nn.Module):\n#     def __init__(self, num_layers=3, hidden_size=768, num_attention_heads=8, dropout_rate=0.1):\n#         super(CustomDualBERTModel, self).__init__()\n        \n#         # Two BERT models: one for each input\n#         self.bert1 = AutoModel.from_pretrained('bert-base-uncased')\n#         self.bert2 = AutoModel.from_pretrained('bert-base-uncased')\n        \n#         self.encoder_layers = nn.ModuleList([\n#             CustomEncoderLayer(hidden_size, num_attention_heads, dropout_rate)\n#             for _ in range(num_layers)\n#         ])\n        \n#         self.fc = nn.Linear(hidden_size * 2, 2)\n\n#     def forward(self, input_ids1, attention_mask1, input_ids2, attention_mask2):\n#         # Get BERT embeddings from two different models\n#         output1 = self.bert1(input_ids=input_ids1, attention_mask=attention_mask1).last_hidden_state\n#         output2 = self.bert2(input_ids=input_ids2, attention_mask=attention_mask2).last_hidden_state\n        \n#         # Create attention masks\n#         x_mask = ~attention_mask1.bool()\n#         y_mask = ~attention_mask2.bool()\n        \n#         # Apply custom encoder layers (self-attention and cross-attention)\n#         for layer in self.encoder_layers:\n#             output1, _ = layer(output1, output2)\n#             output2, cross_attn_weights = layer(output2, output1)\n#         # Pool the outputs from both BERTs\n#         pooled_output1 = torch.mean(output1, dim=1)\n#         pooled_output2 = torch.mean(output2, dim=1)\n        \n#         # Combine and classify\n#         combined = torch.cat((pooled_output1, pooled_output2), dim=1)\n#         return self.fc(combined)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-21T10:32:21.416668Z","iopub.execute_input":"2025-02-21T10:32:21.416951Z","iopub.status.idle":"2025-02-21T10:32:21.420813Z","shell.execute_reply.started":"2025-02-21T10:32:21.416926Z","shell.execute_reply":"2025-02-21T10:32:21.419964Z"},"trusted":true},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizer, AdamW,AutoModel,AutoTokenizer\nfrom tqdm import tqdm  # Import tqdm for progress bars\n\n# Hyperparameters\nMAX_LEN = 128\nBATCH_SIZE = 16\nEPOCHS = 2\nLEARNING_RATE = 2e-5\n\n# tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n\n# Create data loaders\ntrain_dataset = MultiFeatureDataset(train_df, tokenizer, MAX_LEN)\nval_dataset = MultiFeatureDataset(eval_df, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n\n# Initialize the model\n\n# Usage\n# model = CustomDualBERTModel()\n#model.load_state_dict(torch.load('dual_bert_classifier.pth'))\n\n\n# Optimizer with weight decay\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:21.672918Z","iopub.execute_input":"2025-02-21T10:32:21.673143Z","iopub.status.idle":"2025-02-21T10:32:21.688414Z","shell.execute_reply.started":"2025-02-21T10:32:21.673125Z","shell.execute_reply":"2025-02-21T10:32:21.687497Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"optimizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:21.881839Z","iopub.execute_input":"2025-02-21T10:32:21.882113Z","iopub.status.idle":"2025-02-21T10:32:21.886655Z","shell.execute_reply.started":"2025-02-21T10:32:21.882090Z","shell.execute_reply":"2025-02-21T10:32:21.885993Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"AdamW (\nParameter Group 0\n    betas: (0.9, 0.999)\n    correct_bias: True\n    eps: 1e-06\n    lr: 2e-05\n    weight_decay: 0.01\n)"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"import gc\nimport torch\n\n# Delete any unnecessary objects, e.g., big tensors or models you no longer need\n# del model  \n\n# Run garbage collector\ngc.collect()\n\n# If you’re using PyTorch, clear GPU cache\ntorch.cuda.empty_cache()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:22.051800Z","iopub.execute_input":"2025-02-21T10:32:22.052071Z","iopub.status.idle":"2025-02-21T10:32:22.441363Z","shell.execute_reply.started":"2025-02-21T10:32:22.052050Z","shell.execute_reply":"2025-02-21T10:32:22.440523Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:25.986826Z","iopub.execute_input":"2025-02-21T10:32:25.987127Z","iopub.status.idle":"2025-02-21T10:32:25.994358Z","shell.execute_reply.started":"2025-02-21T10:32:25.987105Z","shell.execute_reply":"2025-02-21T10:32:25.993637Z"}},"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"CustomDualBanglaBERTModel(\n  (bert1): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (bert2): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (encoder_layers): ModuleList(\n    (0-1): 2 x CustomEncoderLayer(\n      (self_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (cross_attention): MultiheadAttention(\n        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n      )\n      (feed_forward): Sequential(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): ReLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n      )\n      (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (layer_norm3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (fc): Linear(in_features=1536, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":55},{"cell_type":"code","source":"# Custom function to save the model\ndef save_model(model, filepath, epoch, val_accuracy, best_val_accuracy):\n    \"\"\"\n    Save the model if the current validation accuracy is higher than the best seen so far.\n    \"\"\"\n    if val_accuracy > best_val_accuracy:\n        torch.save(model.state_dict(), filepath)\n        print(f\"Model saved at epoch {epoch+1} with validation accuracy: {val_accuracy:.2f}%\")\n        return val_accuracy  # Update best validation accuracy\n    return best_val_accuracy  # Keep the previous best validation accuracy\n\n# Training loop with model saving\nbest_val_accuracy = 0.0  # Initialize the best validation accuracy\nmodel_filepath = 'dual_bert_classifier_best.pth'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:32:26.192460Z","iopub.execute_input":"2025-02-21T10:32:26.192734Z","iopub.status.idle":"2025-02-21T10:32:26.197327Z","shell.execute_reply.started":"2025-02-21T10:32:26.192714Z","shell.execute_reply":"2025-02-21T10:32:26.196427Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# TRaining LOOP\nfor epoch in range(EPOCHS):\n    model.train()\n    running_loss = 0.0\n    correct_train = 0\n    total_train = 0\n\n    # Initialize the tqdm progress bar\n    progress_bar = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\", leave=False)\n    \n    for batch_idx, batch in enumerate(progress_bar):\n        text_input_ids1 = batch['input_ids1'].to(device)\n        text_attention_mask1 = batch['attention_mask1'].to(device)\n        text_input_ids2 = batch['input_ids2'].to(device)\n        text_attention_mask2 = batch['attention_mask2'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        outputs = model(text_input_ids1, text_attention_mask1, text_input_ids2, text_attention_mask2)\n        \n        # Calculate loss\n        loss = nn.CrossEntropyLoss()(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        \n        # Update running loss and accuracy\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total_train += labels.size(0)\n        correct_train += (predicted == labels).sum().item()\n        \n        avg_loss = running_loss / (batch_idx + 1)\n\n        # Update the progress bar with the average loss\n        progress_bar.set_postfix(avg_loss=avg_loss)\n\n    # Optional: Print statistics at the end of each epoch\n    epoch_loss = running_loss / len(train_loader)\n    epoch_acc = correct_train / total_train\n    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n    \n    # Validation\n    model.eval()\n    val_loss = 0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\", leave=False):\n            text_input_ids1 = batch['input_ids1'].to(device)\n            text_attention_mask1 = batch['attention_mask1'].to(device)\n            text_input_ids2 = batch['input_ids2'].to(device)\n            text_attention_mask2 = batch['attention_mask2'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(text_input_ids1, text_attention_mask1, text_input_ids2, text_attention_mask2)\n            val_loss += nn.CrossEntropyLoss()(outputs, labels).item()\n            _, predicted = torch.max(outputs.data, 1)\n            total_val += labels.size(0)\n            correct_val += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * correct_val / total_val\n    print(f'Epoch {epoch+1}/{EPOCHS}, Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n\n    # Save the model if validation accuracy improves\n    best_val_accuracy = save_model(model, model_filepath, epoch, val_accuracy, best_val_accuracy)","metadata":{"execution":{"iopub.status.busy":"2025-02-21T10:32:28.618413Z","iopub.execute_input":"2025-02-21T10:32:28.618741Z","iopub.status.idle":"2025-02-21T10:34:07.697211Z","shell.execute_reply.started":"2025-02-21T10:32:28.618711Z","shell.execute_reply":"2025-02-21T10:34:07.696499Z"},"trusted":true},"outputs":[{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [1/2], Loss: 0.6347, Accuracy: 0.6250\n","output_type":"stream"},{"name":"stderr","text":"                                                                   \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1/2, Validation Loss: 0.7048, Validation Accuracy: 67.50%\nModel saved at epoch 1 with validation accuracy: 67.50%\n","output_type":"stream"},{"name":"stderr","text":"                                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Epoch [2/2], Loss: 0.3361, Accuracy: 0.8688\n","output_type":"stream"},{"name":"stderr","text":"                                                                   ","output_type":"stream"},{"name":"stdout","text":"Epoch 2/2, Validation Loss: 0.7821, Validation Accuracy: 66.50%\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Define the model checkpoint and path to the saved weights\nmodel_checkpoint = 'csebuetnlp/banglabert'\nsaved_model_path = 'dual_bert_classifier_best.pth'\n\n# Recreate the model architecture\nmodel = CustomDualBanglaBERTModel(model_checkpoint=model_checkpoint)\n\n# Load the model weights from the saved file\nmodel.load_state_dict(torch.load(saved_model_path))\n\n# Send the model to the appropriate device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nprint(\"Custom model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:34:07.698425Z","iopub.execute_input":"2025-02-21T10:34:07.698678Z","iopub.status.idle":"2025-02-21T10:34:09.963506Z","shell.execute_reply.started":"2025-02-21T10:34:07.698656Z","shell.execute_reply":"2025-02-21T10:34:09.962729Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-58-699ca1464688>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(saved_model_path))\n","output_type":"stream"},{"name":"stdout","text":"Custom model loaded successfully!\n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"\nfrom tqdm import tqdm\n\ndef batch_predict(loader):\n    model.eval()  # Set the model to evaluation mode\n    predictions = []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Batch Prediction\", leave=False):\n            text_input_ids1 = batch['input_ids1'].to(device)\n            text_attention_mask1 = batch['attention_mask1'].to(device)\n            text_input_ids2 = batch['input_ids2'].to(device)\n            text_attention_mask2 = batch['attention_mask2'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(text_input_ids1, text_attention_mask1, text_input_ids2, text_attention_mask2)\n            _, predicted = torch.max(outputs.data, 1)\n            predictions.extend(predicted.cpu().numpy())\n\n    return predictions\npred=batch_predict(val_loader)\ntrue=val_dataset.labels\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, accuracy_score\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support\nimport numpy as np\nfrom sklearn.preprocessing import label_binarize\n\n\n# Generate predictions and true labels\n\npred = np.array(pred)\ntrue_labels =np.array(true)\n\n\n\n# Calculate precision, recall, and F1 score for each class\nprecision, recall, f1, _ = precision_recall_fscore_support(true_labels, pred, average=None)\n\n# Get the unique labels\nlabels =np.array([0,1])\n\n# Print the precision, recall, and F1 score for each class\nfor i, label in enumerate(labels):\n    print(f\"Class {label}:\")\n    print(f\"  Precision: {precision[i]:.4f}\")\n    print(f\"  Recall:    {recall[i]:.4f}\")\n    print(f\"  F1 Score:  {f1[i]:.4f}\")\n\n# Optionally, you can calculate the average scores\navg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(true_labels, pred, average='weighted')\n\nprint(f\"\\nAverage Precision: {avg_precision:.4f}\")\nprint(f\"Average Recall:    {avg_recall:.4f}\")\nprint(f\"Average F1 Score:  {avg_f1:.4f}\")\n\n# Compute accuracy\naccuracy = accuracy_score(true_labels, pred)\nprint(f'Accuracy: {accuracy:.4f}')\n\n# Print the number of samples for each class and total samples\nunique, counts = np.unique(true_labels, return_counts=True)\nclass_distribution = dict(zip(unique, counts))\ntotal_samples = len(true_labels)\nprint(f'Class Distribution: {class_distribution}')\nprint(f'Total Samples: {total_samples}')\n\n# Compute confusion matrix\ncm = confusion_matrix(true_labels, pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n\n# Plot confusion matrix\ndisp.plot(cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:34:09.964787Z","iopub.execute_input":"2025-02-21T10:34:09.965045Z","iopub.status.idle":"2025-02-21T10:34:13.792857Z","shell.execute_reply.started":"2025-02-21T10:34:09.965022Z","shell.execute_reply":"2025-02-21T10:34:13.791651Z"}},"outputs":[{"name":"stderr","text":"                                                                 \r","output_type":"stream"},{"name":"stdout","text":"Class 0:\n  Precision: 0.6220\n  Recall:    0.8229\n  F1 Score:  0.7085\nClass 1:\n  Precision: 0.7671\n  Recall:    0.5385\n  F1 Score:  0.6328\n\nAverage Precision: 0.6975\nAverage Recall:    0.6750\nAverage F1 Score:  0.6691\nAccuracy: 0.6750\nClass Distribution: {0: 96, 1: 104}\nTotal Samples: 200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"eval_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T10:34:13.793956Z","iopub.execute_input":"2025-02-21T10:34:13.794374Z","iopub.status.idle":"2025-02-21T10:34:13.809648Z","shell.execute_reply.started":"2025-02-21T10:34:13.794335Z","shell.execute_reply":"2025-02-21T10:34:13.808481Z"}},"outputs":[{"execution_count":60,"output_type":"execute_result","data":{"text/plain":"          id                                     parent_comment  label  \\\n12480  31004                                   এত হাই হলে হবে ?      1   \n13973   3495  টাকা দেয়া বন্ধ করে দেন চোর গুলো সোজা হয়ে যাব...      0   \n21961  20930                       এইরকম চুপিসারে কাজ করা ভাল ।      0   \n\n                                                 comment  \n12480                                   ? হবে হলে হাই এত  \n13973  । সিদ্ধান্ত সঠিক । হবে উন্নতি সরকারের করবে কাজ...  \n21961                       । ভাল করা কাজ চুপিসারে এইরকম  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>parent_comment</th>\n      <th>label</th>\n      <th>comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>12480</th>\n      <td>31004</td>\n      <td>এত হাই হলে হবে ?</td>\n      <td>1</td>\n      <td>? হবে হলে হাই এত</td>\n    </tr>\n    <tr>\n      <th>13973</th>\n      <td>3495</td>\n      <td>টাকা দেয়া বন্ধ করে দেন চোর গুলো সোজা হয়ে যাব...</td>\n      <td>0</td>\n      <td>। সিদ্ধান্ত সঠিক । হবে উন্নতি সরকারের করবে কাজ...</td>\n    </tr>\n    <tr>\n      <th>21961</th>\n      <td>20930</td>\n      <td>এইরকম চুপিসারে কাজ করা ভাল ।</td>\n      <td>0</td>\n      <td>। ভাল করা কাজ চুপিসারে এইরকম</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":60},{"cell_type":"code","source":"# def preprocess_function(context,text):\n#     parent_encoding = tokenizer(context, padding=\"max_length\", truncation=True, max_length=128)\n#     text_encoding = tokenizer(context, padding=\"max_length\", truncation=True, max_length=128)\n    \n#     return {\n#         'input_ids1': parent_encoding['input_ids'],\n#         'attention_mask1': parent_encoding['attention_mask'],\n#         'input_ids2': text_encoding['input_ids'],\n#         'attention_mask2': text_encoding['attention_mask'],\n#     }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#batch=preprocess_function(data2.iloc[1011]['parent_comment'],data2.iloc[1011]['comment'])","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}